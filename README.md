# LLaMA-Paper-List

Collection of paper using LLaMA as backbone model.

## Table of Contents

- [About LLaMA and related theory](#about-llama-and-related-theory)
- [LLaMA with parameter efficiency](#llama-with-parameter-efficiency)
- [LLaMA in Real world application](#llama-in-real-world-application)

## Papers

### About LLaMA and related theory

- **LLaMA: Open and Efficient Foundation Language Models.** arxiv 2023. [paper](https://arxiv.org/abs/2302.13971). [code](https://github.com/facebookresearch/llama/tree/main)<br />
*YHugo Touvron, Thibaut Lavril, Gautier Izacard, Xavier Martinet, Marie-Anne Lachaux, Timothée Lacroix, Baptiste Rozière, Naman Goyal, Eric Hambro, Faisal Azhar, Aurelien Rodriguez, Armand Joulin, Edouard Grave, Guillaume Lample*
- **Training Compute-Optimal Large Language Models.** NeurIPS 2022. [paper](https://arxiv.org/abs/2203.15556).<br />
*Jordan Hoffmann, Sebastian Borgeaud, Arthur Mensch, Elena Buchatskaya, Trevor Cai, Eliza Rutherford, Diego de Las Casas, Lisa Anne Hendricks, Johannes Welbl, Aidan Clark, Tom Hennigan, Eric Noland, Katie Millican, George van den Driessche, Bogdan Damoc, Aurelia Guy, Simon Osindero, Karen Simonyan, Erich Elsen, Jack W. Rae, Oriol Vinyals, Laurent Sifre*

### LLaMA with parameter efficiency

- **LLaMA-Adapter: Efficient Fine-tuning of Language Models with Zero-init Attention.** arxiv 2023. [paper](https://arxiv.org/abs/2303.16199). [code](https://github.com/ZrrSkywalker/LLaMA-Adapter)<br />
*Zhang, Renrui and Han, Jiaming and Zhou, Aojun and Hu, Xiangfei and Yan, Shilin and Lu, Pan and Li, Hongsheng and Gao, Peng and Qiao, Yu*
- **LLaMA-Adapter V2: Parameter-Efficient Visual Instruction Model.** arxiv 2023. [paper](https://arxiv.org/abs/2304.15010). [code](https://github.com/ZrrSkywalker/LLaMA-Adapter)<br />
*Peng Gao, Jiaming Han, Renrui Zhang, Ziyi Lin, Shijie Geng, Aojun Zhou, Wei Zhang, Pan Lu, Conghui He, Xiangyu Yue, Hongsheng Li, Yu Qiao*
- **LLM-Adapters: An Adapter Family for Parameter-Efficient Fine-Tuning of Large Language Models.** arxiv 2023. [paper](https://arxiv.org/abs/2304.01933).<br />
*Zhiqiang Hu, Yihuai Lan, Lei Wang, Wanyu Xu, Ee-Peng Lim, Roy Ka-Wei Lee, Lidong Bing, Xing Xu, Soujanya Poria*

### LLaMA in Real world application

- **ChatDoctor: A Medical Chat Model Fine-Tuned on a Large Language Model Meta-AI (LLaMA) Using Medical Domain Knowledge.**. arxiv 2023. [paper](https://arxiv.org/abs/2303.14070).<br />
*Yunxiang Li, Zihan Li, Kai Zhang, Ruilong Dan, Steve Jiang, You Zhang*
- **Efficient and Effective Text Encoding for Chinese LLaMA and Alpaca.** arxiv 2023. [paper](https://arxiv.org/abs/2304.08177). [code](https://github.com/ymcui/Chinese-LLaMA-Alpaca)<br />
*Yiming Cui, Ziqing Yang, Xin Yao*
- **PMC-LLaMA: Further Finetuning LLaMA on Medical Papers.** arxiv 2023. [paper](https://arxiv.org/abs/2304.14454).<br />
*Chaoyi Wu, Xiaoman Zhang, Ya Zhang, Yanfeng Wang, Weidi Xie*
- **Dr. LLaMA: Improving Small Language Models on PubMedQA
via Generative Data Augmentation.** arxiv 2023. [paper](https://arxiv.org/abs/2305.07804).<br />
*Zhen Guo, Peiqi Wang, Yanwei Wang, Shangdi Yu*
- **Goat: Fine-tuned LLaMA Outperforms GPT-4 on Arithmetic Tasks.** arxiv 2023. [paper](https://arxiv.org/abs/2305.14201).<br />
*Tiedong Liu, Bryan Kian Hsiang Low*

## How to contribute

Contributions are welcome! Please refer to [CONTRIBUTING.md](CONTRIBUTING.md) for contribution guidelines.